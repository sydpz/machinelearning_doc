---
title: 机器学习--线性回归 
tags: 机器学习, 回归
grammar_cjkRuby: true
---
本文是对由Stanford大学Andrew Ng讲授的机器学习课程进行个人心得总结。

---

在机器学习中，回归算法是入门级的课程，但由于其较易理解，该算法也是生产环境中使用最为广泛的算法。本文将从线性回归开始一步一步讲述作者对回归算法的理解。

### 问题的引出

线性回归中常提到的一个例子是房价问题，比如说如下是最近北京地区交易的房屋成交价列表

| 面积 | 价格 |
| ---- | ---- |
| 90   | 183  |
| 54   | 104  |
| 60   | 114  |
| 200  | 390  |
| 130  | 254  |
| 140  | 279  |
| 70   | 143  |
| 110  | 210  |
| 86   |   153   |

目前我们想出售一个面积为100平的房屋，想问一下该房屋应该如何估价较为合理。
将该数据映射到X-Y轴上制成散列图，在图中可以看出数据目前可能满足线性分布(实际情况下房价和房屋面积之间并不是线性分布，当前我们仅以次为例)，故当前我们假设房价和房屋面积之间的关系为$\ y = A * x + b $
这个过程可用下图表示：
![这里写图片描述](http://img.blog.csdn.net/20160505231054643)

接下来我们的工作变为求解 a 和 b 的值，然后利用得到的a和b的值去预估我们想要出售的100平的房屋售价为多少。该逻辑可用下图表示：
![这里写图片描述](http://img.blog.csdn.net/20160505231111488)

为了将该讲解变得更有普遍性，我们将输入的“房屋面积”认为是数据的特征(feature), 输出的价格认为是目标值(target)，而将 $\ a * X + b $ 认为是假设模型(hypothesis), $\ y = h(x) $ ; 
另外，我们会将训练集合的数据个数(即上文用于拟合曲线的样本数)认为是 特征的维数。

###线性回归模型
这里讨论线性回归模型时会注重一些通用性。
  在之前我们讨论的内容里所有的输入的特征维度只是一维(房屋的面积)， 但真实生活中我们遇到的问题可能更多的是多个特征存在的情况。 比如我们出售房屋时不仅仅有房屋的面积，可能还包括 税负等等， 假设税负是我们需要考虑的第二个维度。 则当前我们的输入特征x是一个二维矢量:其中, $\ x_1 $表示房子的面积，$\ x_2 $表示房屋的税负, 则
$\ x_1^{(i)} $表示 第$\ i$个房子的面积；
$\ x_2^{(i)} $表示 第$\ i$个房子的税负；
于是我们假设输入特征x与房价y之间满足的线性函数应该为：
$\ h_θ (x)= Θ_0+ Θ_1 x_1+ Θ_2 x_2 $

$\ Θ_i$即为输入特征x与结果y的线性函数h的相关参数， 这里我们可以假设存在$\ x_0 =1 $,  则该公式可以简化为
$\ h_θ (x)= ∑_{(i=0)}^n(Θ_i x_i )= Θ^T x $
以上是对线性回归模型的定义和假设，目前我们得到了h_θ (x)的表达式，在接下的工作中我们需要做的是通过运算获得各个Θ_i的具体值。在线性回归中，我们定义了一个代价函数， 该函数用来描述h(x) 和 y 之间的接近程度:
$\ J(Θ)= 1/2 ∑_{i=0}^n[(h_θ (x^{(i)})-y^{(i)})]^2 $
该公式本质上就是不停的求$\ h_θ(x) $ 与y之间的差值，当该差值最小时，我们此时认为$\ h_θ (x) $能最好的预测y的值。

 基于该公式，我们推出了下一步的研究方向：梯度下降法。
